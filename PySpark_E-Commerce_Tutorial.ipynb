{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67a4296b",
   "metadata": {},
   "source": [
    "# PySpark E-Commerce Tutorial üöÄ\n",
    "\n",
    "This notebook demonstrates how to use **PySpark** to process and analyze e-commerce data in a clean, production-oriented way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60b474",
   "metadata": {},
   "source": [
    "## üéØ Objectives\n",
    "- Initialize a Spark Session correctly\n",
    "- Load raw CSV data into Spark DataFrames\n",
    "- Explore schemas and validate data quality\n",
    "- Apply transformations and aggregations using PySpark APIs\n",
    "- Write clean, readable, and well-documented PySpark code\n",
    "\n",
    "This notebook is suitable for:\n",
    "- Beginners learning PySpark fundamentals\n",
    "- Junior ‚Üí Mid Data Engineers\n",
    "- Anyone preparing for Data Engineering interviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea552828",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942557c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session\n",
    "# appName: Logical name for the Spark application\n",
    "# master: local[*] means use all available CPU cores\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName('e-commerce-analysis')\n",
    "    .master('local[*]')\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print('Spark Version:', spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2379d4f8",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load Customers Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4d243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read customers data from CSV file\n",
    "# header=True      -> First row contains column names\n",
    "# inferSchema=True -> Spark automatically infers column data types\n",
    "\n",
    "df_customers = (\n",
    "    spark.read\n",
    "    .option('header', True)\n",
    "    .option('inferSchema', True)\n",
    "    .csv('customers.csv')\n",
    ")\n",
    "\n",
    "# Inspect schema to understand data structure\n",
    "df_customers.printSchema()\n",
    "\n",
    "# Preview a sample record\n",
    "df_customers.show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e665a",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Orders Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read orders dataset\n",
    "\n",
    "df_orders = (\n",
    "    spark.read\n",
    "    .option('header', True)\n",
    "    .option('inferSchema', True)\n",
    "    .csv('orders.csv')\n",
    ")\n",
    "\n",
    "# Check schema and sample rows\n",
    "df_orders.printSchema()\n",
    "df_orders.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2395450",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Basic Data Exploration üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07255a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of customers\n",
    "customers_count = df_customers.count()\n",
    "\n",
    "# Total number of orders\n",
    "orders_count = df_orders.count()\n",
    "\n",
    "print(f'Total Customers: {customers_count}')\n",
    "print(f'Total Orders: {orders_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c94f36",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Orders by Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Count number of orders per order_status\n",
    "orders_by_status = (\n",
    "    df_orders\n",
    "    .groupBy('order_status')\n",
    "    .count()\n",
    "    .orderBy(col('count').desc())\n",
    ")\n",
    "\n",
    "orders_by_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c0519",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Join Customers with Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join orders with customers on customer_id\n",
    "\n",
    "df_orders_customers = (\n",
    "    df_orders\n",
    "    .join(df_customers, on='customer_id', how='inner')\n",
    ")\n",
    "\n",
    "# Validate join result\n",
    "df_orders_customers.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49d3f8",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Business Insight Example üí°\n",
    "### Number of Orders per Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate orders per customer\n",
    "orders_per_customer = (\n",
    "    df_orders_customers\n",
    "    .groupBy('customer_id')\n",
    "    .count()\n",
    "    .withColumnRenamed('count', 'total_orders')\n",
    "    .orderBy(col('total_orders').desc())\n",
    ")\n",
    "\n",
    "orders_per_customer.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ffa37",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Performance Notes ‚öôÔ∏è\n",
    "- Spark uses **lazy evaluation**, so transformations run only when an action is triggered\n",
    "- Use `.select()` early to reduce data shuffling\n",
    "- Partitioning & caching significantly improve performance at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c598e2",
   "metadata": {},
   "source": [
    "## ‚úÖ Conclusion\n",
    "In this notebook, we covered essential PySpark concepts used in real-world Data Engineering:\n",
    "- Reading structured data\n",
    "- Schema exploration\n",
    "- Aggregations and joins\n",
    "- Writing clean and maintainable Spark code\n",
    "\n",
    "### üîú Next Steps\n",
    "- Add partitioning and caching strategies\n",
    "- Store processed data in Parquet format\n",
    "- Load final datasets into a Data Warehouse (PostgreSQL / BigQuery)\n",
    "\n",
    "üìå *Learning PySpark is a journey ‚Äî consistency and practice make the difference.*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
